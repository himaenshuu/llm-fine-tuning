# ðŸš€ LLM Fine-Tuning with Unsloth + Hugging Face

> Fine-tune large language models (LLMs) like Mistral or LLaMA efficiently on custom instruction datasets using [Unsloth](https://github.com/unslothai/unsloth), [Hugging Face Transformers](https://huggingface.co/docs/transformers), and export to GGUF for fast inference via `llama.cpp`.

![Python](https://img.shields.io/badge/python-3.10+-blue)

![Status](https://img.shields.io/badge/status-active-success)

---

## ðŸ“Œ Project Summary

This project demonstrates how to fine-tune an LLM on a custom instruction-based dataset using:

- ðŸ§  **Unsloth** for memory-efficient fine-tuning
- ðŸ”§ **TRLâ€™s `SFTTrainer`** from Hugging Face for supervised training
- ðŸ’¾ **GGUF Export** for inference-ready deployment (supports `llama.cpp`, `llamafile`, etc.)
- ðŸ“Š Optional **W&B tracking** for experiment visualization

---

## ðŸ§± Tech Stack

| Component       | Tool/Library               |
|-----------------|----------------------------|
| Model           | Mistral / LLaMA            |
| Trainer         | TRL's `SFTTrainer`         |
| Optimization    | AdamW (8-bit), LR Schedulers |
| Quantization    | 8-bit / 4-bit via GGUF     |
| Logging         | Weights & Biases (optional) |
| Hardware Target | Colab / Kaggle GPU         |

---

## âœ… Features

- ðŸ”§ Fine-tunes **Mistral** or **LLaMA** models with minimal VRAM requirements
- ðŸ§  Supports **instruction tuning** for domain-specific and structured tasks
- âš¡ Trains with **8-bit optimizer** using `bitsandbytes` for faster and lighter execution
- ðŸ“¦ Exports the final model in **GGUF format** compatible with `llama.cpp`, `llamafile`, etc.
- ðŸŽ¯ Runs on **Kaggle**, **Google Colab**, or custom local GPU environments
- ðŸ“Š Optional **Weights & Biases (W&B)** logging for real-time experiment tracking

---

## ðŸ§ª Training Configuration

| Hyperparameter        | Value                |
|------------------------|----------------------|
| Epochs                | 2â€“3                  |
| Batch Size            | 2 (with accumulation = 8) |
| Max Steps             | 100                  |
| Learning Rate         | 2e-4                 |
| Optimizer             | AdamW (8-bit)        |
| Precision             | fp16 / bf16          |
| Quantization          | GGUF export (8-bit)  |

---

