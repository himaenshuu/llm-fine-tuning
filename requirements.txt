# Core LLM fine-tuning & Hugging Face ecosystem
transformers==4.40.0
datasets==2.18.0
trl==0.7.10
accelerate==0.27.2
peft==0.10.0
bitsandbytes==0.43.1

# Unsloth: lightweight fine-tuning wrapper
unsloth==2024.4.15

# Evaluation and utilities
scikit-learn>=1.1.0
tqdm>=4.65.0
numpy>=1.23.0

# Optional: W&B for logging
wandb>=0.16.0

# Torch version (compatible with CUDA 11.8+)
torch==2.1.2
